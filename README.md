# NBA Play-By-Play Analysis

This is a project for a master's degree university course in big data that focuses on batch and stream processing using Spark.

Batch processing was implemented using MongoDB for data storage, Airflow for orchestration and PySpark.

Stream processing was implemented using Delta Lake for data storage, Airflow for orchestration, Apache Kafka for asynchronous communication between services and PySpark.

<hr>
Dejan DopuÄ‘, E2 2/2023, FTN